{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import openai\n",
    "import whisper\n",
    "from whisper.utils import format_timestamp\n",
    "from sentence_transformers import SentenceTransformer, util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_key = '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' # os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_transcript_str(whisper_out):\n",
    "        \n",
    "    transcript_str = []\n",
    "    for segment in whisper_out['segments']:\n",
    "        transcript_str.append(f\"[{format_timestamp(segment['start'])}]:\\t{segment['text']}\")\n",
    "    \n",
    "    transcript_str = \"\\n\".join(transcript_str)\n",
    "    return transcript_str\n",
    "\n",
    "def postprocess_points(raw_output):\n",
    "    points = raw_output.split('\\n-')\n",
    "    points = [point.strip() for point in points]\n",
    "    points = [point for point in points if point != '']\n",
    "    return points\n",
    "\n",
    "def find_closest_time(time, all_times):\n",
    "    closest_time = min(all_times, key=lambda x: abs(x - time))\n",
    "    return closest_time\n",
    "\n",
    "def create_transcript_chunks(all_start_times, all_end_times, whisper_out, stride=45, length=60):\n",
    "    '''Create larger chunks of the segments using a sliding window'''\n",
    "\n",
    "    transcript_chunks = []\n",
    "    for seek in range(0, int(all_end_times[-1]), stride):\n",
    "        chunk = {'start': None, 'end': None, 'text': None}\n",
    "\n",
    "        start_index = all_start_times.index(find_closest_time(seek, all_start_times))\n",
    "        chunk['start'] = all_start_times[start_index]\n",
    "        end_index = all_end_times.index(find_closest_time(seek + length, all_end_times))\n",
    "        chunk['end'] = all_end_times[end_index]\n",
    "\n",
    "        chunk['text'] = \"\".join([segment['text'] for segment in whisper_out['segments'][start_index:end_index+1]]).strip()\n",
    "\n",
    "        transcript_chunks.append(chunk)\n",
    "    \n",
    "    return transcript_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LISAPipeline():\n",
    "    def __init__(self, whisper_model, search_model):\n",
    "        self.whisper_model = whisper.load_model(whisper_model)\n",
    "        self.search_model = SentenceTransformer.load(search_model)\n",
    "    \n",
    "    def run_gpt3(self, prompt, max_tokens=256, temperature=0.5, top_p=1, frequency_penalty=0.0, presence_penalty=0.0):\n",
    "        response = openai.Completion.create(\n",
    "            engine=\"text-davinci-002\",\n",
    "            prompt=prompt,\n",
    "            max_tokens=max_tokens,\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "            frequency_penalty=frequency_penalty,\n",
    "            presence_penalty=presence_penalty\n",
    "        )\n",
    "        return response.choices[0].text\n",
    "    \n",
    "    def transcribe(self, audio_path):\n",
    "        whisper_out = self.whisper_model.transcribe(audio_path, verbose=False)\n",
    "        return whisper_out\n",
    "    \n",
    "    def minutes_of_meeting(self, transcript_str):\n",
    "\n",
    "        mom_prompt = f\"\"\"Generate the minutes of the meeting for the following transcript:\n",
    "        Meeting Transcription:\n",
    "        {transcript_str}\n",
    "\n",
    "        Meeting Minutes:\n",
    "        -\"\"\"\n",
    "\n",
    "        raw_minutes = '\\n-' + self.run_gpt3(mom_prompt, temperature=0.5)\n",
    "        minutes = postprocess_points(raw_minutes)\n",
    "\n",
    "        return minutes\n",
    "\n",
    "    def action_items(self, transcript_str):\n",
    "\n",
    "        if 'hey lisa' not in transcript_str.lower():\n",
    "            return []\n",
    "\n",
    "        action_prompt = f\"\"\"Extract the Action Items / To-Do List from the Transcript.\n",
    "        Meeting Transcription:\n",
    "        {transcript_str}\n",
    "\n",
    "        Action Items:\n",
    "        -\"\"\"\n",
    "        raw_action_items = self.run_gpt3(action_prompt, temperature=0.4)\n",
    "        action_items = postprocess_points(raw_action_items)\n",
    "\n",
    "        return action_items\n",
    "\n",
    "    def create_index(self, whisper_out):\n",
    "        '''Create search index by embedding the transcript segments'''\n",
    "        all_start_times = [segment['start'] for segment in whisper_out['segments']]\n",
    "        all_end_times = [segment['end'] for segment in whisper_out['segments']]\n",
    "\n",
    "        transcript_chunks = create_transcript_chunks(all_start_times, all_end_times, whisper_out, stride=45, length=60)\n",
    "\n",
    "        # Encode query and documents\n",
    "        chunk_texts = [chunk['text'] for chunk in transcript_chunks]\n",
    "        doc_emb = self.search_model.encode(chunk_texts)\n",
    "\n",
    "        return doc_emb, transcript_chunks\n",
    "    \n",
    "    def search(self, doc_embeddings, transcript_chunks, query, top_k=3, threshold=16):\n",
    "        # Compute dot score between query and all document embeddings\n",
    "        query_embeddings = self.search_model.encode(query)\n",
    "        scores = util.dot_score(query_embeddings, doc_embeddings)[0].cpu().tolist()\n",
    "\n",
    "        chunks = [(chunk['start'], chunk['end'], chunk['text']) for chunk in transcript_chunks]\n",
    "\n",
    "        # Combine docs & scores\n",
    "        chunk_score_tuples = [(*chunks[i], scores[i]) for i in range(len(chunks))]\n",
    "\n",
    "        # Sort by decreasing score\n",
    "        chunk_score_tuples = sorted(chunk_score_tuples, key=lambda x: x[-1], reverse=True)\n",
    "\n",
    "        # Output passages & scores\n",
    "        results = []\n",
    "        for start, end, text, score in chunk_score_tuples[:top_k]:\n",
    "            if score > threshold:\n",
    "                results.append((start, end, text))\n",
    "\n",
    "        return results\n",
    "\n",
    "    def __call__(self, audio_path):\n",
    "        '''Run the pipeline on an audio file'''\n",
    "        whisper_out = self.transcribe(audio_path)\n",
    "        transcript_str = create_transcript_str(whisper_out)\n",
    "        minutes = self.minutes_of_meeting(transcript_str)\n",
    "        action_items = self.action_items(transcript_str)\n",
    "        doc_emb, transcript_chunks = self.create_index(whisper_out)\n",
    "\n",
    "        return minutes, action_items, doc_emb, transcript_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "lisa = LISAPipeline(whisper_model=\"whisper_models/medium.pt\", search_model=\"multi-qa-mpnet-base-dot-v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minutes, action_items, doc_emb, transcript_chunks = lisa('Recording.m4a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected language: English\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20870/20870 [00:36<00:00, 574.00frames/s]\n"
     ]
    }
   ],
   "source": [
    "whisper_out = lisa.transcribe('Recording.m4a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcript_str = create_transcript_str(whisper_out)\n",
    "minutes = lisa.minutes_of_meeting(transcript_str)\n",
    "action_items = lisa.action_items(transcript_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_emb, transcript_chunks = lisa.create_index(whisper_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The final module of the course on Neuro-Symbolic AI is discussed.',\n",
       " 'The contents of the session are reviewed.',\n",
       " 'The problem is introduced.',\n",
       " 'A review of symbolic AI is conducted.',\n",
       " 'The differences between neural networks and symbolic AI are explored.',\n",
       " 'The architecture of neuro-symbolic AI is explained.']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(133.2,\n",
       "  192.39999999999998,\n",
       "  \"So maybe like water and person. So it might say there's water, okay. So the deep learning model might just say water and there are people in it, so it will either say people or person. But apart from that, if we humans are given a definition of a flood, then we will actually be able to reason about an image and get the flood answer, right? So as a human, what we do, we also see both water and people there, right? But we also see that there's water person, but also we see that it's not really a beach or anything, right? It's not like because even in a beach, people are there as well as water is there. But here it doesn't really look like a beach, there are trees and everyone is just submerged in and so on, right? So based on that reasoning that it's not a beach or a pool, we can like conclude like that it's a flood, right? So this is how we humans reason about it. But deep learning\"),\n",
       " (181.4,\n",
       "  218.92000000000002,\n",
       "  \"right? So based on that reasoning that it's not a beach or a pool, we can like conclude like that it's a flood, right? So this is how we humans reason about it. But deep learning doesn't do that. It only does what it is told to do. It cannot extrapolate and find new patterns or find new information, just reason about something new that it was not trained on unless we didn't learn it too.\"),\n",
       " (89.32000000000001,\n",
       "  150.88,\n",
       "  \"of flavor data set again, but a bit of a modified version that I created for neuro-symbolic AI. So we'll look at that and then we move on to the actual architecture and all the modules and everything that we'll be utilizing. So the entire work will start from here. So I guess we had already looked at this particular problem a while back, I think in one of the first few modules, I guess. So basically what we have here is basically an image of a flood, right? So if we give a deep learning model, which was not trained on floods or anything, it would obviously, a deep learning model would only like predict a few labels, I guess. So maybe like water and person. So it might say there's water, okay. So the deep learning model might just say water and there are people in it, so it will either say people or person. But apart from that, if we humans are given a definition of a flood, then we will actually\")]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lisa.search(doc_emb, transcript_chunks, 'explain the issue with deep learning with an example')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('ai')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "66c34959fd061e0b75fe61b25789565ee0f65e3017c38ac9c89fb2c0e5483aee"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
