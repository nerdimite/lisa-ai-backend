{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import openai\n",
    "import whisper\n",
    "from whisper.utils import format_timestamp\n",
    "from sentence_transformers import SentenceTransformer, util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_key = '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' # os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_transcript_str(whisper_out):\n",
    "    \n",
    "    transcript_str = []\n",
    "    for segment in whisper_out['segments']:\n",
    "        transcript_str.append(f\"[{format_timestamp(segment['start'])}]:\\t{segment['text']}\")\n",
    "    \n",
    "    transcript_str = \"\\n\".join(transcript_str)\n",
    "    return transcript_str\n",
    "\n",
    "def postprocess_points(raw_output):\n",
    "    points = raw_output.split('\\n-')\n",
    "    points = [point.strip() for point in points]\n",
    "    points = [point for point in points if point != '']\n",
    "    return points\n",
    "\n",
    "def find_closest_time(time, all_times):\n",
    "    closest_time = min(all_times, key=lambda x: abs(x - time))\n",
    "    return closest_time\n",
    "\n",
    "def create_transcript_chunks(all_start_times, all_end_times, whisper_out, stride=45, length=60):\n",
    "    '''Create larger chunks of the segments using a sliding window'''\n",
    "\n",
    "    transcript_chunks = []\n",
    "    for seek in range(0, int(all_end_times[-1]), stride):\n",
    "        chunk = {'start': None, 'end': None, 'text': None}\n",
    "\n",
    "        start_index = all_start_times.index(find_closest_time(seek, all_start_times))\n",
    "        chunk['start'] = all_start_times[start_index]\n",
    "        end_index = all_end_times.index(find_closest_time(seek + length, all_end_times))\n",
    "        chunk['end'] = all_end_times[end_index]\n",
    "\n",
    "        chunk['text'] = \"\".join([segment['text'] for segment in whisper_out['segments'][start_index:end_index+1]]).strip()\n",
    "\n",
    "        transcript_chunks.append(chunk)\n",
    "    \n",
    "    return transcript_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LISAPipeline():\n",
    "    def __init__(self, whisper_model, search_model):\n",
    "        self.whisper_model = whisper.load_model(whisper_model)\n",
    "        self.search_model = SentenceTransformer.load(search_model)\n",
    "    \n",
    "    def run_gpt3(self, prompt, max_tokens=256, temperature=0.5, top_p=1, frequency_penalty=0.0, presence_penalty=0.0):\n",
    "        response = openai.Completion.create(\n",
    "            engine=\"text-davinci-002\",\n",
    "            prompt=prompt,\n",
    "            max_tokens=max_tokens,\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "            frequency_penalty=frequency_penalty,\n",
    "            presence_penalty=presence_penalty\n",
    "        )\n",
    "        return response.choices[0].text\n",
    "    \n",
    "    def transcribe(self, audio_path):\n",
    "        whisper_out = self.whisper_model.transcribe(audio_path, verbose=False)\n",
    "        return whisper_out\n",
    "    \n",
    "    def minutes_of_meeting(self, transcript_str):\n",
    "\n",
    "        mom_prompt = f\"\"\"Generate the minutes of the meeting for the following transcript:\n",
    "        Meeting Transcription:\n",
    "        {transcript_str}\n",
    "\n",
    "        Meeting Minutes:\n",
    "        -\"\"\"\n",
    "\n",
    "        raw_minutes = '\\n-' + self.run_gpt3(mom_prompt, temperature=0.5)\n",
    "        minutes = postprocess_points(raw_minutes)\n",
    "\n",
    "        return minutes\n",
    "\n",
    "    def action_items(self, transcript_str):\n",
    "\n",
    "        if 'hey lisa' not in transcript_str.lower():\n",
    "            return []\n",
    "\n",
    "        action_prompt = f\"\"\"Extract the Action Items / To-Do List from the Transcript.\n",
    "        Meeting Transcription:\n",
    "        {transcript_str}\n",
    "\n",
    "        Action Items:\n",
    "        -\"\"\"\n",
    "        raw_action_items = self.run_gpt3(action_prompt, temperature=0.4)\n",
    "        action_items = postprocess_points(raw_action_items)\n",
    "\n",
    "        return action_items\n",
    "\n",
    "    def create_index(self, whisper_out):\n",
    "        '''Create search index by embedding the transcript segments'''\n",
    "        all_start_times = [segment['start'] for segment in whisper_out['segments']]\n",
    "        all_end_times = [segment['end'] for segment in whisper_out['segments']]\n",
    "\n",
    "        transcript_chunks = create_transcript_chunks(all_start_times, all_end_times, whisper_out, stride=45, length=60)\n",
    "\n",
    "        # Encode query and documents\n",
    "        chunk_texts = [chunk['text'] for chunk in transcript_chunks]\n",
    "        doc_emb = self.search_model.encode(chunk_texts)\n",
    "\n",
    "        return doc_emb, transcript_chunks\n",
    "    \n",
    "    def search(self, doc_embeddings, transcript_chunks, query, top_k=3, threshold=16):\n",
    "        # Compute dot score between query and all document embeddings\n",
    "        query_embeddings = self.search_model.encode(query)\n",
    "        scores = util.dot_score(query_embeddings, doc_embeddings)[0].cpu().tolist()\n",
    "\n",
    "        chunks = [(chunk['start'], chunk['end'], chunk['text']) for chunk in transcript_chunks]\n",
    "\n",
    "        # Combine docs & scores\n",
    "        chunk_score_tuples = [(*chunks[i], scores[i]) for i in range(len(chunks))]\n",
    "\n",
    "        # Sort by decreasing score\n",
    "        chunk_score_tuples = sorted(chunk_score_tuples, key=lambda x: x[-1], reverse=True)\n",
    "\n",
    "        # Output passages & scores\n",
    "        results = []\n",
    "        for start, end, text, score in chunk_score_tuples[:top_k]:\n",
    "            if score > threshold:\n",
    "                results.append((start, end, text))\n",
    "\n",
    "        return results\n",
    "\n",
    "    def __call__(self, audio_path):\n",
    "        '''Run the pipeline on an audio file'''\n",
    "        whisper_out = self.transcribe(audio_path)\n",
    "        transcript_str = create_transcript_str(whisper_out)\n",
    "        minutes = self.minutes_of_meeting(transcript_str)\n",
    "        action_items = self.action_items(transcript_str)\n",
    "        doc_emb, transcript_chunks = self.create_index(whisper_out)\n",
    "\n",
    "        return minutes, action_items, doc_emb, transcript_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "lisa = LISAPipeline(whisper_model=\"whisper_models/medium.pt\", search_model=\"multi-qa-mpnet-base-dot-v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected language: English\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 418/418 [00:02<00:00, 164.41frames/s]\n"
     ]
    }
   ],
   "source": [
    "minutes, action_items, doc_emb, transcript_chunks = lisa('Recording2.mp3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "np.array(doc_emb.tolist()) == doc_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "whisper_out = lisa.transcribe('Recording.m4a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcript_str = create_transcript_str(whisper_out)\n",
    "minutes = lisa.minutes_of_meeting(transcript_str)\n",
    "action_items = lisa.action_items(transcript_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_emb, transcript_chunks = lisa.create_index(whisper_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Bhavish introduced himself.']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lisa.search(doc_emb, transcript_chunks, 'explain the issue with deep learning with an example')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import moviepy.editor as mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clip = mp.VideoFileClip('Recording.mp4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clip.audio.write_audiofile(\"Recording2.mp3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('ai')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "66c34959fd061e0b75fe61b25789565ee0f65e3017c38ac9c89fb2c0e5483aee"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
