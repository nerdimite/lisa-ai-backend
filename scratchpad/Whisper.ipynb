{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import whisper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = whisper.load_model(\"medium\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detecting language using up to the first 30 seconds. Use `--language` to specify the language\n",
      "Detected language: English\n",
      "[00:00.000 --> 00:11.240]  So, hello everybody. Welcome to the final module of this course on Neuro-Symbolic AI.\n",
      "[00:11.240 --> 00:20.400]  So in the past few, like in the past, we have like, in the previous session, we looked at\n",
      "[00:20.400 --> 00:26.260]  symbolic AI and we looked at basically, propositional logic, first order logic and program synthesis.\n",
      "[00:26.260 --> 00:30.760]  So the program synthesis part is going to be something that we are going to use today.\n",
      "[00:30.760 --> 00:35.520]  And we also looked at relation networks last time, although it was just like an overview\n",
      "[00:35.520 --> 00:46.800]  of that. So today we will conclude by learning about neuro-symbolic AI. So let's get started\n",
      "[00:46.800 --> 00:47.800]  here.\n",
      "[00:47.800 --> 01:10.760]  Okay. So the contents for this session are going to be basically, we'll start of course\n",
      "[01:10.760 --> 01:16.120]  by talking about the problem. We'll again have a very quick review of symbolic AI, although\n",
      "[01:16.120 --> 01:22.360]  we just looked at, but just a very quick review of symbolic AI from last time. And we also\n",
      "[01:22.360 --> 01:29.320]  have like a difference between neural networks and symbolic AI. And then we look at the sort\n",
      "[01:29.320 --> 01:34.040]  of flavor data set again, but a bit of a modified version that I created for neuro-symbolic\n",
      "[01:34.040 --> 01:39.400]  AI. So we'll look at that and then we move on to the actual architecture and all the\n",
      "[01:39.400 --> 01:45.120]  modules and everything that we'll be utilizing. So the entire work will start from here.\n",
      "[01:45.120 --> 01:55.080]  So I guess we had already looked at this particular problem a while back, I think in one of the\n",
      "[01:55.080 --> 02:01.960]  first few modules, I guess. So basically what we have here is basically an image of a flood,\n",
      "[02:01.960 --> 02:06.840]  right? So if we give a deep learning model, which was not trained on floods or anything,\n",
      "[02:06.840 --> 02:13.200]  it would obviously, a deep learning model would only like predict a few labels, I guess.\n",
      "[02:13.200 --> 02:17.680]  So maybe like water and person. So it might say there's water, okay. So the deep learning\n",
      "[02:17.680 --> 02:25.320]  model might just say water and there are people in it, so it will either say people or person.\n",
      "[02:25.320 --> 02:30.880]  But apart from that, if we humans are given a definition of a flood, then we will actually\n",
      "[02:30.880 --> 02:37.520]  be able to reason about an image and get the flood answer, right? So as a human, what we\n",
      "[02:37.520 --> 02:48.200]  do, we also see both water and people there, right? But we also see that there's water\n",
      "[02:48.200 --> 02:52.600]  person, but also we see that it's not really a beach or anything, right? It's not like\n",
      "[02:52.600 --> 02:56.920]  because even in a beach, people are there as well as water is there. But here it doesn't\n",
      "[02:56.920 --> 03:01.400]  really look like a beach, there are trees and everyone is just submerged in and so on,\n",
      "[03:01.400 --> 03:07.480]  right? So based on that reasoning that it's not a beach or a pool, we can like conclude\n",
      "[03:07.480 --> 03:12.400]  like that it's a flood, right? So this is how we humans reason about it. But deep learning\n",
      "[03:12.400 --> 03:19.480]  doesn't do that. It only does what it is told to do. It cannot extrapolate and find new\n",
      "[03:19.480 --> 03:25.280]  patterns or find new information, just reason about something new that it was not trained\n",
      "51054 50364\n",
      "[03:25.280 --> 03:39.080]  on unless we didn't learn it too.\n"
     ]
    }
   ],
   "source": [
    "result = model.transcribe(\"Recording.m4a\", verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 1,\n",
       " 'seek': 0,\n",
       " 'start': 11.24,\n",
       " 'end': 20.400000000000002,\n",
       " 'text': ' So in the past few, like in the past, we have like, in the previous session, we looked at',\n",
       " 'tokens': [50364,\n",
       "  407,\n",
       "  11,\n",
       "  7751,\n",
       "  2201,\n",
       "  13,\n",
       "  4027,\n",
       "  281,\n",
       "  264,\n",
       "  2572,\n",
       "  10088,\n",
       "  295,\n",
       "  341,\n",
       "  1164,\n",
       "  322,\n",
       "  1734,\n",
       "  7052,\n",
       "  12,\n",
       "  50,\n",
       "  88,\n",
       "  5612,\n",
       "  299,\n",
       "  7318,\n",
       "  13,\n",
       "  50926,\n",
       "  50926,\n",
       "  407,\n",
       "  294,\n",
       "  264,\n",
       "  1791,\n",
       "  1326,\n",
       "  11,\n",
       "  411,\n",
       "  294,\n",
       "  264,\n",
       "  1791,\n",
       "  11,\n",
       "  321,\n",
       "  362,\n",
       "  411,\n",
       "  11,\n",
       "  294,\n",
       "  264,\n",
       "  3894,\n",
       "  5481,\n",
       "  11,\n",
       "  321,\n",
       "  2956,\n",
       "  412,\n",
       "  51384,\n",
       "  51384,\n",
       "  25755,\n",
       "  7318,\n",
       "  293,\n",
       "  321,\n",
       "  2956,\n",
       "  412,\n",
       "  1936,\n",
       "  11,\n",
       "  7532,\n",
       "  2628,\n",
       "  9952,\n",
       "  11,\n",
       "  700,\n",
       "  1668,\n",
       "  9952,\n",
       "  293,\n",
       "  1461,\n",
       "  30252,\n",
       "  13,\n",
       "  51677,\n",
       "  51677],\n",
       " 'temperature': 0.0,\n",
       " 'avg_logprob': -0.28447111991986834,\n",
       " 'compression_ratio': 1.538888888888889,\n",
       " 'no_speech_prob': 0.059526924043893814}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result['segments'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('result.json', 'w') as f:\n",
    "    json.dump(result, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('ai')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "66c34959fd061e0b75fe61b25789565ee0f65e3017c38ac9c89fb2c0e5483aee"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
